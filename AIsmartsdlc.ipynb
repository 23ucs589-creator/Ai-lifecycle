{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n5_t_UuFhjzF"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import PyPDF2\n",
        "import io\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"ibm-granite/granite-3.2-2b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_response(prompt, max_length=1024):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.replace(prompt, \"\").strip()\n",
        "    return response\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    if pdf_file is None:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "def requirement_analysis(pdf_file, prompt_text):\n",
        "    # Get text from PDF or prompt\n",
        "    if pdf_file is not None:\n",
        "        content = extract_text_from_pdf(pdf_file)\n",
        "        analysis_prompt = f\"Analyze the following document and extract key software requirements. Organize them into functional requirements, non-functional requirements, and technical specifications:\\n\\n{content}\"\n",
        "    else:\n",
        "        analysis_prompt = f\"Analyze the following requirements and organize them into functional requirements, non-functional requirements, and technical specifications:\\n\\n{prompt_text}\"\n",
        "\n",
        "    return generate_response(analysis_prompt, max_length=1200)\n",
        "\n",
        "def code_generation(prompt, language):\n",
        "    code_prompt = f\"Generate {language} code for the following requirement:\\n\\n{prompt}\\n\\nCode:\"\n",
        "    return generate_response(code_prompt, max_length=1200)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# AI Code Analysis & Generator\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Code Analysis\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "                    prompt_input = gr.Textbox(\n",
        "                        label=\"Or write requirements here\",\n",
        "                        placeholder=\"Describe your software requirements...\",\n",
        "                        lines=5\n",
        "                    )\n",
        "                    analyze_btn = gr.Button(\"Analyze\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    analysis_output = gr.Textbox(label=\"Requirements Analysis\", lines=20)\n",
        "\n",
        "            analyze_btn.click(requirement_analysis, inputs=[pdf_upload, prompt_input], outputs=analysis_output)\n",
        "\n",
        "        with gr.TabItem(\"Code Generation\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    code_prompt = gr.Textbox(\n",
        "                        label=\"Code Requirements\",\n",
        "                        placeholder=\"Describe what code you want to generate...\",\n",
        "                        lines=5\n",
        "                    )\n",
        "                    language_dropdown = gr.Dropdown(\n",
        "                        choices=[\"Python\", \"JavaScript\", \"Java\", \"C++\", \"C#\", \"PHP\", \"Go\", \"Rust\"],\n",
        "                        label=\"Programming Language\",\n",
        "                        value=\"Python\"\n",
        "                    )\n",
        "                    generate_btn = gr.Button(\"Generate Code\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    code_output = gr.Textbox(label=\"Generated Code\", lines=20)\n",
        "\n",
        "            generate_btn.click(code_generation, inputs=[code_prompt, language_dropdown], outputs=code_output)\n",
        "\n",
        "app.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}